<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Michele Antonazzi </title> <meta name="author" content="Michele Antonazzi"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="micheleantonazzi, michele, antonazzi, michele antonazzi"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon_ma.png?eeecec437b937d4e7eb0a4fcad761d49"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://micheleantonazzi.github.io/publications/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Michele Antonazzi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">RAS</abbr> </div> <div id="LUPERTO2025105137" class="col-sm-10"> <div class="title">Multi-robot rendezvous in communication-restricted unknown environments via backtracking and semantic frontier-based exploration</div> <div class="author"> Matteo Luperto, Mauro Tellaroli, <em>Michele Antonazzi</em>, and Nicola Basilico </div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://aislab.di.unimi.it/research/fberendezvous/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.1016/j.robot.2025.105137" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>The multi-robot rendezvous problem requires coordinating a team of mobile robots to converge at a common location. Efficient decentralized execution with limited communication and in unknown environments can be essential in several applications, but such real-world robotic features are often not captured by theoretical rendezvous frameworks. In this work, we present an approach to this problem that extends traditional frontier-based exploration strategies to facilitate efficient rendezvous in such conditions. Our method allows robots to backtrack their exploration of the environment and exploit semantic knowledge on the map by prioritizing high-connectivity areas like corridors and hallways. We define and evaluate different variants of our method to study a trade-off between the time taken to perform a rendezvous and the amount of discovered area in the environment. Extensive experimental evaluation in ROS using 3D simulations demonstrates the feasibility of our method and its performance improvements over baselines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">LUPERTO2025105137</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-robot rendezvous in communication-restricted unknown environments via backtracking and semantic frontier-based exploration}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Autonomous Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{105137}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0921-8890}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.robot.2025.105137}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luperto, Matteo and Tellaroli, Mauro and Antonazzi, Michele and Basilico, Nicola}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">ESWA</abbr> </div> <div id="LUIS2025126483" class="col-sm-10"> <div class="title">Variational model-based Deep Reinforcement Learning for Non-Homogeneous Patrolling aquatic environments with multiple unmanned surface vehicles</div> <div class="author"> Samuel Yanes Luis, Nicola Basilico, <em>Michele Antonazzi</em>, Daniel Gutiérrez-Reina, and Sergio Toral Marín </div> <div class="periodical"> <em>Expert Systems with Applications</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://doi.org/10.1016/j.eswa.2025.126483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper addresses the challenge of Non-Homogeneous Patrolling for Autonomous Surface Vehicles in non-homogeneous importance water environments with a dissimilar biological monitorization criterion. Traditional monitoring methods fail, especially in expansive areas such as Lake Ypacaraíin Paraguay. The proposed solution employs a cooperative Deep Reinforcement Learning framework, specifically a multi-agent version of the Double Deep Q-Learning algorithm based on safe-consensus decision making. This framework optimizes adaptive policies for such vehicles by simultaneously modeling the environment and patrolling high-importance zones. The incorporation of a Variational Auto-Encoder based on the U-Network architecture directly addresses the non-observability of the environment by predicting biological importance from partial observations. The methodology is validated in a realistic algae bloom contamination scenario, demonstrating superior performance and computational efficiency compared to traditional approaches like Gaussian Processes and K-Nearest-Neighbors. The Deep Reinforcement Learning framework, coupled with the Variational Auto-Encoder model, showcases flexibility and efficiency in addressing multi-agent cooperation and long-term objective optimization for water quality monitoring. The results reveal significant improvements, with the proposed model exceeding well-founded approaches with a 30% faster minimization of the patrolling score compared to these methods.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="antonazzi2024privacyweakloss" class="col-sm-10"> <div class="title">Privacy-Preserving Robotic Perception for Object Detection in Curious Cloud Robotics</div> <div class="author"> <em>Michele Antonazzi</em>, Matteo Alberti, Alex Bassot, Matteo Luperto, and Nicola Basilico </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> (Under Review) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://aislab.di.unimi.it/research/privacyweakloss/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>Cloud robotics allows low-power robots to perform computationally intensive inference tasks by offloading them to the cloud, raising privacy concerns when transmitting sensitive images. Although end-to-end encryption secures data in transit, it doesn’t prevent misuse by inquisitive third-party services since data must be decrypted for processing. This paper tackles these privacy issues in cloud-based object detection tasks for service robots. We propose a co-trained encoder-decoder architecture that retains only task-specific features while obfuscating sensitive information, utilizing a novel weak loss mechanism with proposal selection for privacy preservation. A theoretical analysis of the problem is provided, along with an evaluation of the trade-off between detection accuracy and privacy preservation through extensive experiments on public datasets and a real robot.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">antonazzi2024privacyweakloss</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Privacy-Preserving Robotic Perception for Object Detection in Curious Cloud Robotics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Antonazzi, Michele and Alberti, Matteo and Bassot, Alex and Luperto, Matteo and Basilico, Nicola}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{(Under Review)}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="antonazzi2024r2snet" class="col-sm-10"> <div class="title">R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement</div> <div class="author"> <em>Michele Antonazzi</em>, Matteo Luperto, N. Alberto Borghese, and Nicola Basilico </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://aislab.di.unimi.it/research/r2snet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.1109/IROS58592.2024.10802847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="video btn btn-sm z-depth-0" role="button">Video</a> <a href="/assets/pdf/IROS-2024-R2SNet-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks. Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals. Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs. Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">antonazzi2024r2snet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Antonazzi, Michele and Luperto, Matteo and Borghese, N. Alberto and Basilico, Nicola}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="video hidden"> <div style="text-align: center;"> <figure> <video src="/assets/video/r2snet_video_extended.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls="" muted=""></video> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="tellaroli2024frontierbased" class="col-sm-10"> <div class="title">Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments</div> <div class="author"> Mauro Tellaroli, Matteo Luperto, <em>Michele Antonazzi</em>, and Nicola Basilico </div> <div class="periodical"> <em>In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://aislab.di.unimi.it/research/fberendezvous/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.1109/IROS58592.2024.10801321" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="video btn btn-sm z-depth-0" role="button">Video</a> <a href="/assets/pdf/2024IROS_FBRrendezvous_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">tellaroli2024frontierbased</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tellaroli, Mauro and Luperto, Matteo and Antonazzi, Michele and Basilico, Nicola}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="video hidden"> <div style="text-align: center;"> <figure> <video src="/assets/video/frontierbased_exploration.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls="" muted=""></video> </figure> </div> </div> </div> </div> </li> <li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">arXiv</abbr> </div> <div id="antonazzi2024development" class="col-sm-10"> <div class="title">Development and Adaptation of Robotic Vision in the Real-World: the Challenge of Door Detection</div> <div class="author"> <em>Michele Antonazzi</em>, Matteo Luperto, N. Alberto Borghese, and Nicola Basilico </div> <div class="periodical"> <em></em> 2024 </div> <div class="periodical"> (Under Review) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://aislab.di.unimi.it/research/doordetection/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://doi.org/10.48550/arXiv.2401.17996" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>Mobile service robots are increasingly prevalent in human-centric, real-world domains, operating autonomously in unconstrained indoor environments. In such a context, robotic vision plays a central role in enabling service robots to perceive high-level environmental features from visual observations. Despite the data-driven approaches based on deep learning push the boundaries of vision systems, applying these techniques to real-world robotic scenarios presents unique methodological challenges. Traditional models fail to represent the challenging perception constraints typical of service robots and must be adapted for the specific environment where robots finally operate. We propose a method leveraging photorealistic simulations that balances data quality and acquisition costs for synthesizing visual datasets from the robot perspective used to train deep architectures. Then, we show the benefits in qualifying a general detector for the target domain in which the robot is deployed, showing also the trade-off between the effort for obtaining new examples from such a setting and the performance gain. In our extensive experimental campaign, we focus on the door detection task (namely recognizing the presence and the traversability of doorways) that, in dynamic settings, is useful to infer the topology of the map. Our findings are validated in a real-world robot deployment, comparing prominent deep-learning models and demonstrating the effectiveness of our approach in practical settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">antonazzi2024development</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Development and Adaptation of Robotic Vision in the Real-World: the Challenge of Door Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Antonazzi, Michele and Luperto, Matteo and Borghese, N. Alberto and Basilico, Nicola}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{(Under Review)}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">ECMR</abbr> </div> <div id="antonazzi2023enhancing" class="col-sm-10"> <div class="title">Enhancing Door-Status Detection for Autonomous Mobile Robots during Environment-Specific Operational Use</div> <div class="author"> <em>Michele Antonazzi</em>, Matteo Luperto, Nicola Basilico, and N. Alberto Borghese </div> <div class="periodical"> <em>In 2023 European Conference on Mobile Robots (ECMR)</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://doi.org/10.1109/ECMR59166.2023.10256289" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="video btn btn-sm z-depth-0" role="button">Video</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>Door-status detection, namely recognising the presence of a door and its status (open or closed), can induce a remarkable impact on a mobile robot’s navigation performance, especially for dynamic settings where doors can enable or disable passages, changing the topology of the map. In this work, we address the problem of building a door-status detector module for a mobile robot operating in the same environment for a long time, thus observing the same set of doors from different points of view. First, we show how to improve the mainstream approach based on object detection by considering the constrained perception setup typical of a mobile robot. Hence, we devise a method to build a dataset of images taken from a robot’s perspective and we exploit it to obtain a door-status detector based on deep learning. We then leverage the typical working conditions of a robot to qualify the model for boosting its performance in the working environment via fine-tuning with additional data. Our experimental analysis shows the effectiveness of this method with results obtained both in simulation and in the real-world, that also highlights a trade-off between the costs and benefits of the fine-tuning approach.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">antonazzi2023enhancing</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Enhancing Door-Status Detection for Autonomous Mobile Robots during Environment-Specific Operational Use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Antonazzi, Michele and Luperto, Matteo and Basilico, Nicola and Borghese, N. Alberto}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 European Conference on Mobile Robots (ECMR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> <div class="video hidden"> <div style="text-align: center;"> <figure> <img src="/assets/video/ecmr_door_detection.gif" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto"> </figure> </div> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-lg-1 abbr"> <abbr class="badge">AURO</abbr> </div> <div id="luperto2020robot" class="col-sm-10"> <div class="title">Robot exploration of indoor environments using incomplete and inaccurate prior knowledge</div> <div class="author"> Matteo Luperto, <em>Michele Antonazzi</em>, Francesco Amigoni, and N. Alberto Borghese </div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">abstract</a> <a href="https://doi.org/10.1016/j.robot.2020.103622" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Cite</a> </div> <div class="abstract hidden"> <p>Exploration is a task in which autonomous mobile robots incrementally discover features of interest in initially unknown environments. We consider the problem of exploration for map building, in which a robot explores an indoor environment in order to build a metric map. Most of the current exploration strategies used to select the next best locations to visit ignore prior knowledge about the environments to explore that, in some practical cases, could be available. In this paper, we present an exploration strategy that evaluates the amount of new areas that can be perceived from a location according to a priori knowledge about the structure of the indoor environment being explored, like the floor plan or the contour of external walls. Although this knowledge can be incomplete and inaccurate (e.g., a floor plan typically does not represent furniture and objects and consequently may not fully mirror the structure of the real environment), we experimentally show, both in simulation and with real robots, that employing prior knowledge improves the exploration performance in a wide range of settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">luperto2020robot</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robot exploration of indoor environments using incomplete and inaccurate prior knowledge}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luperto, Matteo and Antonazzi, Michele and Amigoni, Francesco and Borghese, N. Alberto}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics and Autonomous Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{133}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103622}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.robot.2020.103622}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Michele Antonazzi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?3ccd542d421f0416edd0b3ed4c9aecb8"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>